\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[babel,english=british]{csquotes} % cool quotes
\usepackage[backend=biber,style=ieee]{biblatex} % bibliogrpahy
\usepackage[utf8]{inputenc}
\usepackage{pgf}
\usepackage{tikz}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[pagebackref=false,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,draft]{hyperref}

%\let\cite\parencite
\addbibresource{literature.bib}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Using Reinforcement Learning to play Connect Four}

\author{Raphael Michel
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Lucas-Raphael Müller
\and
Florian Störtz
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Tbd.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

In this project, we try to use methods from Reinforcement Learning to solve
the well-known game \emph{Connect Four}, in which two players take turns in
inserting coins from the top into the columns of a $7\times 6$ grid board.
The first player who is able to establish four of their coins consecutively
within a column, row, or diagonal, wins the game.

It has been shown by Edelkamp and Kissmann that there are exactly $4\,531\,985\,219\,092$ legal positions in this game \cite{Edelkamp2008}. In the 1980s, two authors independently found a strategy for perfect playing of the game \cite{Allis88}\cite{Allen1990} that is quite complicated to perform. The game has since also been solved by brute force and and strong solvers have been created using minimax or negamax methods.

Our interest in this project is whether Reinforcement Learning methods will allow us to solve the game with less effort -- either less computational effort or less effort in algorithm design and implementation.

Additionally, we implement a traditional vision algorithm with the aim to
recognize the current game state of a Connect Four board from a photo of the
board. This could be seen as a proof of concept for creating an end-user friendly smartphone application that assists in playing the game.

\section{Methods}

\subsection{Vision}
We provide a framework which features extraction of game information(i.e. board configuration) by processing image data.
The board undergoes a variety of image processing steps and is then identified and checked.
In case the former process failed, the user is asked to retake the photograph.

\subsubsection{Object Detection using Haar-Like Features}
The very first step is detection of the connect4 board by using a Haar-Like feature incorporating Haar wavelet functions.
This processing step is done within the openCV framework \parencite{openCV}.
After the feature has been trained, the framework features the output of a \textit{Haar cascade xml-file} for fast object detection.

\paragraph{Training} has been performed on a set of 25 \textit{distinct} pictures wich were taken by hand (20) and from the internet (5).
Training of a Haar cascade needs many (in the order of thousands positive and negative samples;  \cite{kuranov}).




 by color and pattern comparision.
The image is then cropped and undergoes various image processing steps, such as wrapping, contrast enhancement and color channel extraction.
The current board configuration can then be read and converted to a computer readable format by thresholded color comparision.

\subsection{State representation}
Theoretically, we need at least $\lfloor \log_2 3^{6\cdot 7} \rfloor = 67$ bit to store the entire game state, so we could just use a single
128-bit integer to store it.
However, we chose a slightly longer representation by storing the state $s$ in an array of $7$ 16-bit integers $s_i$, one for each column (14 bytes or 112 bit in total).
In each of these integers $s_i$ we assigned two bits for each row of the board, beginning at the lowest two bits with the lowest row.
For every cell, we set the assigned bits to $01$ to represent a coin of player 1, $10$ to represent a coin of player 2 and $00$ to represent an empty cell.

We can now for example efficiently check if a column is full by just checking if the bits of the highest row are set or if the integer value of the column exceeds $0b01000000000$.
Modifying the board and checking for winning positions now just requires some simple bit-wise operations on these integers.

\subsection{Learning}
Our approach to learning the game follows standard reinforcement learning methods.
Our training algorithm performs $N_i$ iterations and simulate $N_g$ games in every iteration.
In each of the simulated games, the moves are determined by a neural network that we will describe in detail below.
The current board state is taken as the input to the neural network and one of the allowed next moves is determined randomly using, weighted by the results of the neural network.
We have only added one single bit of game knowledge to this algorithm: If one of the allowed next moves is a winning move, that move will always be performed.
After the move has ended, a total reward of the end position will be calculated.
We used a reward of $100$ for winning, $-100$ for losing and $-10$ for a draw.

This reward will then be used by the employed algorithms for automatic differentiation and optimization\cite{Adam}, implemented by the software library PyTorch\cite{PyTorch}.

The neural network is constructed of a layer of 42 input neurons, one for each cell of the board, then two layers of 64 neurons each and finally a layer of 8 output neurons, one for each column of the board that the next move could put a coin into.

During the simulations, we experimented with different types of training opponents: random play, random play with winning move recognition and self-play.
In the first case, the opponent just randomly chooses one of the possible moves while in the second case, a random move is chosen except in situations where there is a move that leads to a win of the opponent.
In the third case, the same algorithm and neural network that we train and use for the first player is used for playing both sides (but only the actions of the first player are used for learning).
To avoid the self-play method to just play the same game over and over again, we add some random noise to the decisions of the opponent.

It can be shown that with perfect play on both sides, the first player to insert a coin will always win if and only if they insert their coin in one of the columns adjacent to the middle.
If the first coin is inserted in the outer columns of the game, the second player can win with perfect play.

The consequence of this is that if our learning agent learns the perfect strategy, we would still only see a winning rate of $50\%$ if we randomly
let the agent and its opponent begin.
For simplicity and more understandable results, we therefore assume that our agent always begins the game.

%-------------------------------------------------------------------------
\section{Results}

\subsection{Vision}
Processing time is $< 0.3$ s (i7-4790K CPU @ 4.00GHz) per image.

\subsection{Learning}
Our training algorithm was mainly CPU-bound and using a GPU for tensor
operation did not pose a speed-up since the time wasn't lost in the neural
network computation but in the game logic (e.g. checks for winning position).
We tried to quickly port our Python game logic code to Cython code but were
unable to achieve a speed-up without investing more engineering effort.
In the end, we were able to simulate $N_g = 250$ games and learn from
them within $4\mathrm{s}$ on a fast desktop CPU (i7-4790K @ 4.00 GHz)
or within $8 \mathrm{s}$ on a regular notebook CPU (i5-3210M CPU @ 2.50GHz).

\begin{figure}[t]
    \begin{center}
		\noindent
		\makebox[3.25in]{
	   		\input{../log-250-last_move-self-0.01.pgf}
		}
	\end{center}
    \caption{Play against random}
	\label{fig:long}
\end{figure}

TODO:
number of iterations, number of games
self-play vs random play
noise
reward strategies
%-------------------------------------------------------------------------
\section{Discussion}

{\small
\printbibliography
}

\end{document}
